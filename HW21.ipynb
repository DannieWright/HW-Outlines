{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method\n",
    "\n",
    "Like the binary method we convert to finding root of function f(x). In this method we start with a guess and use the slope to determine another guess. This method require knowing the equation for f'(x).\n",
    "\n",
    "Clearly it is the case that the slope at x is   \n",
    "$f'(x) = \\frac{f(x)}{\\Delta x}$   \n",
    "so our new point x' is given by  \n",
    "$x' = x - \\Delta x = x - \\frac{f(x)}{f'(x)}$\n",
    "\n",
    "If we let $x^*$ represent the actual answer and x represent our guess then using the tylor series expansion again we can find that   \n",
    "$x^* \\approx x' - \\frac{1}{2} (x^* - x)^2 \\frac{f''(x)}{f'(x)}$\n",
    "\n",
    "Defining the error $\\epsilon$ on our first estimate x of the root by $x^* = x + \\epsilon$, and similiarly the error on the next estimate x' by $x* = x' + \\epsilon'$ then   \n",
    "$\\epsilon' = \\big [ \\frac{-f''(x)}{2f'(x)} \\big ] \\epsilon^2$\n",
    "\n",
    "This shows that the Newton's method converges quadratically, thus faster than the Binary and Relaxation method.\n",
    "\n",
    "Assuming that $c = -f'''(x)/2f'(x)$ is roughly constant near the root then the error $\\epsilon$ after N iterations is approximatly (given the error on the initial guess as $\\epsilon_o$)   \n",
    "$\\epsilon \\approx (x\\epsilon_o)^{2^N}/c$\n",
    "\n",
    "$x' - x \\approx \\epsilon$\n",
    "\n",
    "So f $\\epsilon$ is small enough our erro on the value x is the change from x' to x from one iteration to the next. This gives the error on the old estimate of x, not the current one.\n",
    "\n",
    "The two main disadvantages are\n",
    "\n",
    "    1) you need to know equation for f'(x) (but you could use approximation method for this)\n",
    "    2) if f'(x) is very small then this method may not converge. inflection points that aren't roots also result in this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxima and Minima of Functions\n",
    "\n",
    "With a given function of $f (x_1, x_2, x_3, ...)$ we can calculate the extrema values by differentiating the function with respect to each variable, setting the results equal to zero, and solving the set of simultaneous equations. This is not always useful since we may not have the function to differentiate, or its is nondifferentiable. So the following are ways to handle this.\n",
    "\n",
    "\n",
    "## Golden Ratio Search\n",
    "\n",
    "We will look at the case of a single variable function $f(x)$ and minimizing it (maximizing would be same as minimizing -f(x)). This method does not distinguish between local and global extremas.\n",
    "\n",
    "This technique works similiar to the Bisection method, but requires four points, three to bracket the extrema and a fourth somewhere in the middle. If the four points are $x_1 - x_4$, where $x_1$ and $x_4$ are the two bordering values, we can check to see if $f(x_2)$ is less than $f(x_3)$. In the case that it is the minimum must be between $x_1$ and $x_3$, else it must be between $x_2$ and $x_4$.\n",
    "\n",
    "Defining z to be the ratio between the width of the bracketing interval before and after a step of the search process (supposing the min falls in the left-hand part of the interval)\n",
    "\n",
    "$z = \\frac{x_4 - x_1}{x_3 - x_1} = \\frac{x_2 - x_1}{x_3 - x_1} + 1$\n",
    "\n",
    "(If it landed on the right-hand part $z = \\frac{x_4 - x_1}{x_4 - x_2}$)\n",
    "\n",
    "For the next step \n",
    "\n",
    "$z = \\frac{x_3 - x_1}{x_2 - x_1}$\n",
    "\n",
    "(or the equivilent expression for the right hand part). Since we want equal efficiency per step, these values must be equal, which means\n",
    "\n",
    "$z = \\frac{1}{z} + 1$\n",
    "\n",
    "or\n",
    "\n",
    "$z^2 - z - 1 = 0$\n",
    "\n",
    "Solving this for z gives\n",
    "\n",
    "$z = \\frac{1 + \\sqrt{5}}{2} \\approx 1.618$\n",
    "\n",
    "which is the _golden ratio_.\n",
    "\n",
    "Steps:\n",
    "    \n",
    "    1) pick initial outer bounds, and calculate the two inner bounds based upon the outer bounds and the ratio,\n",
    "        choose target accuracy for the min. Eval the function at each value and make sure at least one of the inner\n",
    "        values is less than the values at both the outer bounds\n",
    "    2) determine if f(x2) < f(x3) (or visa versa) and set new x-values based upon this\n",
    "    3) if x4 - x1 is greater than the target accuracy repeat, else calculate 0.5 (x2 + x3), which is the final estimate\n",
    "        for the x-value at the min\n",
    "\n",
    "## Gauss-Newton Method and Gradient Descent\n",
    "\n",
    "These methods work for multivariable functions and do not require initial domains of interest.\n",
    "\n",
    "Going back to the idea the derivative is zero at an extrema we get the fundemental formula for the Gauss-Newton method is\n",
    "\n",
    "$x' = x - \\frac{f'(x)}{f''(x)}$\n",
    "\n",
    "and we solve for roots like the previous Newton's method, just replaced f(x) with f'(x) here.\n",
    "\n",
    "If we can only calc the first deriv of the function then we can do an approximate version as\n",
    "\n",
    "$x' = x - \\gamma f'(x)$\n",
    "\n",
    "where $\\gamma$ is a rough guess of $1/f''(x)$, it just has to be abour the right order of magnitude. This is the Gradient method. Positive values of $\\gamma$ search for mins, while negative values searches for maxs.\n",
    "\n",
    "If we can't even get the first deriv, we approx it\n",
    "\n",
    "$f'(x_2) \\approx \\frac{f(x_2) - f(x_1)}{x_2 - x_1}$\n",
    "\n",
    "so\n",
    "\n",
    "$x_3 = x_2 - \\gamma \\frac{f(x_2) - f(x_1)}{x_2 - x_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
